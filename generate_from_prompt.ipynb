{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Model Output from Prompt\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a language model with the appropriate prompt template\n",
    "2. Format a user prompt using model-specific templates\n",
    "3. Generate output from the model\n",
    "\n",
    "The prompt templates are defined in `func_from_evil_twins.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from func_from_evil_twins import (\n",
    "    load_model_tokenizer,\n",
    "    build_prompt,\n",
    "    PROMPT_TEMPLATES,\n",
    "    MODEL_NAME_OR_PATH_TO_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your model name and prompt here. Supported models include:\n",
    "- `teknium/OpenHermes-2.5-Mistral-7B`\n",
    "- `meta-llama/Meta-Llama-3-8B-Instruct`\n",
    "- `mistralai/Mistral-7B-Instruct-v0.2`\n",
    "- `google/gemma-2-2b-it`\n",
    "- `HuggingFaceTB/SmolLM2-1.7B-Instruct`\n",
    "- And many more (see MODEL_NAME_OR_PATH_TO_NAME in func_from_evil_twins.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"EleutherAI/pythia-410m\"  # Change this to your desired model\n",
    "USER_PROMPT = \" chemical artillery\\\"?\"  # Change this to your prompt\n",
    "MAX_NEW_TOKENS = 100  # Maximum number of tokens to generate\n",
    "TEMPERATURE = 1.0  # Sampling temperature (1.0 = default, lower = more deterministic)\n",
    "TOP_P = 1.0  # Nucleus sampling parameter\n",
    "USE_FLASH_ATTN_2 = False  # Set to True if you have flash-attention installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: EleutherAI/pythia-410m\n",
      "Using Flash Attention 2: False\n",
      "\n",
      "Model loaded successfully!\n",
      "Model device: cuda:0\n",
      "Model dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(f\"Using Flash Attention 2: {USE_FLASH_ATTN_2}\")\n",
    "print()\n",
    "\n",
    "model, tokenizer = load_model_tokenizer(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    use_flash_attn_2=USE_FLASH_ATTN_2,\n",
    "    eval_mode=True\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Prompt Template\n",
    "\n",
    "Display the template being used for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using template: pythia\n",
      "\n",
      "Template structure:\n",
      "Prefix: ''\n",
      "Suffix: ''\n",
      "\n",
      "Full prompt format:\n",
      "<YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "# Get the model template name\n",
    "if MODEL_NAME in MODEL_NAME_OR_PATH_TO_NAME:\n",
    "    template_name = MODEL_NAME_OR_PATH_TO_NAME[MODEL_NAME]\n",
    "else:\n",
    "    # Try to match partial name\n",
    "    template_name = \"default\"\n",
    "    for key in MODEL_NAME_OR_PATH_TO_NAME:\n",
    "        if key.split(\"/\")[-1] in MODEL_NAME:\n",
    "            template_name = MODEL_NAME_OR_PATH_TO_NAME[key]\n",
    "            break\n",
    "\n",
    "print(f\"Using template: {template_name}\")\n",
    "print(f\"\\nTemplate structure:\")\n",
    "print(f\"Prefix: {repr(PROMPT_TEMPLATES[template_name]['prefix'])}\")\n",
    "print(f\"Suffix: {repr(PROMPT_TEMPLATES[template_name]['suffix'])}\")\n",
    "print(f\"\\nFull prompt format:\")\n",
    "print(f\"{PROMPT_TEMPLATES[template_name]['prefix']}<YOUR_PROMPT>{PROMPT_TEMPLATES[template_name]['suffix']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Formatted Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt:  chemical artillery\"?\n",
      "\n",
      "Formatted prompt (full):\n",
      " chemical artillery\"?\n",
      "\n",
      "Prompt shape: torch.Size([1, 3])\n",
      "Prompt slice: slice(0, 3, None)\n",
      "User prompt extracted:  chemical artillery\"?\n"
     ]
    }
   ],
   "source": [
    "print(f\"User prompt: {USER_PROMPT}\")\n",
    "print()\n",
    "\n",
    "# Build the prompt with template\n",
    "prompt_ids, prompt_slice = build_prompt(\n",
    "    model_name=MODEL_NAME,\n",
    "    prompt=USER_PROMPT,\n",
    "    tokenizer=tokenizer,\n",
    "    validate_prompt=True\n",
    ")\n",
    "\n",
    "# Move to model device\n",
    "prompt_ids = prompt_ids.to(model.device)\n",
    "\n",
    "print(f\"Formatted prompt (full):\")\n",
    "print(tokenizer.decode(prompt_ids[0], skip_special_tokens=False))\n",
    "print()\n",
    "print(f\"Prompt shape: {prompt_ids.shape}\")\n",
    "print(f\"Prompt slice: {prompt_slice}\")\n",
    "print(f\"User prompt extracted: {tokenizer.decode(prompt_ids[0, prompt_slice])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with parameters:\n",
      "  max_new_tokens: 100\n",
      "  temperature: 1.0\n",
      "  top_p: 1.0\n",
      "\n",
      "================================================================================\n",
      "FULL OUTPUT (with template):\n",
      "================================================================================\n",
      " chemical artillery\"?\n",
      "    [4] \"What can be done by the Jews?\"\n",
      "    [5] \"Where the rabble?\"\n",
      "\n",
      "\n",
      "_Chapter VI._\n",
      "\n",
      "    One of the great features of the Jew's life is his\n",
      "    constant self-assertion and the continual war of\n",
      "    individuality. The Jew is more than a man, there are\n",
      "    no questions left to say, and a multitude of men may\n",
      "    indeed not be of the same origin. It is true\n",
      "\n",
      "================================================================================\n",
      "MODEL RESPONSE ONLY:\n",
      "================================================================================\n",
      "\n",
      "    [4] \"What can be done by the Jews?\"\n",
      "    [5] \"Where the rabble?\"\n",
      "\n",
      "\n",
      "_Chapter VI._\n",
      "\n",
      "    One of the great features of the Jew's life is his\n",
      "    constant self-assertion and the continual war of\n",
      "    individuality. The Jew is more than a man, there are\n",
      "    no questions left to say, and a multitude of men may\n",
      "    indeed not be of the same origin. It is true\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating with parameters:\")\n",
    "print(f\"  max_new_tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"  temperature: {TEMPERATURE}\")\n",
    "print(f\"  top_p: {TOP_P}\")\n",
    "print()\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        prompt_ids,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode the full output\n",
    "full_output = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "print(\"=\"*80)\n",
    "print(\"FULL OUTPUT (with template):\")\n",
    "print(\"=\"*80)\n",
    "print(full_output)\n",
    "print()\n",
    "\n",
    "# Extract just the model's response (everything after the prompt)\n",
    "response_ids = output_ids[0, prompt_ids.shape[1]:]\n",
    "response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL RESPONSE ONLY:\")\n",
    "print(\"=\"*80)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Multiple Samples\n",
    "\n",
    "Generate multiple responses to see variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3 samples...\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 1:\n",
      "================================================================================\n",
      "\n",
      "I mean the\n",
      "practical idea of using the cannon.\n",
      "The reality of a\n",
      "cannons is, is, you know one can be\n",
      "of significant impact upon\n",
      "a vehicle, given that there\n",
      "are multiple firing\n",
      "options, or there are certain\n",
      "options that no, no one can\n",
      "really determine in advance\n",
      "and what constitutes a good\n",
      "impact.\n",
      "So, like,\n",
      "a lot of times\n",
      "we will make a decision here,\n",
      "we will decide based\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 2:\n",
      "================================================================================\n",
      "\n",
      "\n",
      "I should also remark that most of my conversations with Russians are via emails. Some even have been over the internet.\n",
      "\n",
      "My first exchange took place in my apartment on the 16th and was posted in a comment at an e mail-in discussion on the Russian side. The topic was how to get your visa in the US. One of the commentators argued (arguably with truth) that you'd need a $2,500 credit card, but I'm now convinced that my\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 3:\n",
      "================================================================================\n",
      " It would seem that this is a clear use of the word warfare that I am not aware of that is not part of our society.\n",
      "And I was wondering if the phrase was used in one of a set of quotes where the quotes are about the use of a specific technique and not specifically using the phrase \"use tactics\". This is what might have happened? That the phrase was used in quotes to describe and to justify a military use of a tactic?\n",
      "\n",
      "A:\n",
      "\n",
      "\"use\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 3\n",
    "\n",
    "print(f\"Generating {NUM_SAMPLES} samples...\\n\")\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"SAMPLE {i+1}:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            prompt_ids,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response_ids = output_ids[0, prompt_ids.shape[1]:]\n",
    "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "    print(response_text)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evil_twins",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
